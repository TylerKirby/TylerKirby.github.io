---
title: How Machines Learn
date: "2019-02-23"
template: "post"
draft: false
slug: "/posts/how-machines-learn/"
category: "Machine Learning"
tags:
  - "Theory of Machine Learning"
description: "What does it mean for a machine to learn? I explore this question by analyzing the perceptron learning algorithm."
---
When Google deploys a new spam filter for gmail, how do they know it will work? Of course there are a number of assumptions made in that process, but the foundations of statistical learning are solid and so they can have some confidence in the performance of their algorithm even before it goes live. The bedrock of these foundations are guarantees for the feasibility of learning. But how can we be so certain that an algorithm trained in one environment will perform well in a completely different environment? 


## Feasibility of Learning
Let's first consider the typical formulation of a supervised machine learning problem. Each problem has the same basic elements: some dataset $\mathcal{D}$ with $N$ data points $(x_n, y_n)$ where $x$ is our input and $y$ is our label. Our goal is to search some hypothesis space $\mathcal{H}$ for the function $g(x)$ that best approximates the "true" function $f(x)$. Two measurements are the most important to us: the error rate $E_{in}$ observed on our dataset and the error rate $E_{out}$ of our chosen hypothesis on unseen data. For learning to be feasible, we must prove two points: 1) $E_{in}$ and $E_{out}$ are relatively close together and 2) we can make $E_{in}$ sufficiently small.

Consider a concrete example. We want to predict whether or not a movie will win an Oscar. Our dataset is movies from the last 30 years. We use two primary features: ticket sales and how much was spent to make the movie. Our label is whether or not the movie won an Oscar. Supposed we normalized our features to a mean of 0 and variance of 1. Let's generate some data fitting this requirements and plot the resulting dataset. We'll also make sure that the data is linearly seperable for later.

```python
# Standard Imports
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt

# Set random seed for reproducibility
np.random.seed(41)

# Create input data
N = 100
d = 2
X = np.ones((N,1))
X = np.append(X, -2*np.random.random_sample((N, 2))+1,1)
col_names = [f'x_{i}' for i in range(0,d+1)]

# Ensure data is linearly separable when creating the labels
w_init = np.array([0])
w_init = np.append(w_init, -2*np.random.random_sample((d, 1)))
y = np.sign(np.dot(X, w_init))

# Save data in dataframe
df = pd.DataFrame(columns=col_names, data=X)
df['y'] = y

# Plot data
x = df['x_1']
y = df['x_2']
label = df['y']
plt.figure(figsize=(10,10))
plt.scatter(x, y, c=label)
plt.xlabel('Ticket Sales')
plt.xlim([-1, 1])
plt.ylabel('Production Cost')
plt.ylim([-1, 1])
plt.title('Movies From The Last 30 Years')
```
![mavies_plot.jpg](/media/Movies_plot.jpg)

Seeing that the data is linearly separable, we can use a perceptron to model it. The basic algorithm is this:
1. Add a column of 1s to the data matrix for the bias.
2. Initialize a weight vector $w$ with the same dimensions as the inputs and set $w_0=0$.
3. While the algorithm has not converged:
   1. Determine which inputs are misclassied by computing their labels with the weight vector: $y=sign(X \cdot w)$
   2. If no inputs were misclassified, break and return the weight vector.
   3. Otherwise, select a misclassified input $(x_t, y_t)$ at random.
   4. Update the weight vector accordingly: $w_{t+1} \leftarrow w_t + (y_t \cdot x_t)$

Here's the algorithm implemented in Python:
```python
def pla(df, max_rounds):
    """
    Learn weight vector on data.
    """
    # Select input matrix X and column vector y from dataframe
    X = df.iloc[:,:-1]
    y = df['y']
    
    # Initialize weight vector
    w = np.array([0])
    w = np.append(w, -2*np.random.random_sample((X.shape[1]-1, 1)))
    
    # Terminate if algorithm exceeds the specified max number of iterations
    rounds = 0
    while rounds < max_rounds:
        # Determine misclassified inputs
        y_pred = np.sign(np.dot(X, w))
        errors = [i for i in range(0, len(y)) if y[i] != y_pred[i]]
        # Terminate if no inputs were misclassified
        if len(errors) == 0:
            return w, rounds
        rounds += 1
        # Update weight
        y_t = y[errors[0]]
        x_t = X.iloc[errors[0]].values
        w = w + (y_t * x_t)

    return w, rounds
```
